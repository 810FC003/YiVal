# This is a generated template. Modify the values as needed.

custom_function: demo/auto_reply.reply.reply
dataset:
  file_path: demo/data/test_data.csv
  reader: csv_reader
  source_type: dataset
description: Generated experiment config

# evaluators: []

# wrapper_configs: {}

evaluators:
  - evaluator_type: all
    input_description: Guide GPT-4 to role-play as a predefined fantasy character for a Weibo auto-reply chatbot, ensuring responses align with character traits, backstory, and Chinese cultural context.
    metric_calculators: []
    name: openai_elo_evaluator
    openai_model_name: gpt-4

  - evaluator_type: individual
    metric_calculators:
      - method: AVERAGE
    name: openai_prompt_based_evaluator
    display_name: similarity
    prompt: |-
      You are assessing a submitted answer on a given task based on a criterion. Here is the data:
      - Task: Given a weibo post and a corresponding user input, generate a reasonable response that is similar to the character's persona and Chinese Weibo conversational style.
      - Does the bot's response align with the character's persona presented in weibo post and the conversational style of Chinese Weibo? It should be immediately clear to anyone who reads the reply that the bot is role-playing the character accurately and engaging in a conversation that is typical on Weibo. A lack of similarity can lead to a less immersive and genuine interaction.
      [Input]: {weibo_post}
      [Result]: {raw_output}
      Answer the question by selecting one of the following options:
      A It fails to meet the criterion at all.
      B It somewhat meets the criterion, but there is significant room for improvement.
      C It meets the criterion to a satisfactory degree.
      D It meets the criterion very well.
      E It meets the criterion exceptionally well, with little to no room for improvement.
    choices: ["A", "B", "C", "D", "E"]
    model_name: gpt-4
    description: "Evaluate the similarity of the bot's response to the character's persona and Chinese Weibo conversational style"
    scale_description: "0-4"
    choice_scores:
      A: 0
      B: 1
      C: 2
      D: 3
      E: 4

  - evaluator_type: individual
    metric_calculators:
      - method: AVERAGE
    name: openai_prompt_based_evaluator
    display_name: accuracy
    prompt: |-
      You are assessing a submitted answer on a given task based on a criterion. Here is the data:
      - Task: Given a weibo post and a corresponding user input, generate a reasonable response that is similar to the character's persona and Chinese Weibo conversational style.
      - Does the bot's response accurately address the content of the user input? The bot should be able to understand and respond appropriately to the user's input. An inaccurate response can lead to confusion and a less satisfying user experience.
      [Input]: {user_input}
      [Result]: {raw_output}
      Answer the question by selecting one of the following options:
      A It fails to meet the criterion at all.
      B It somewhat meets the criterion, but there is significant room for improvement.
      C It meets the criterion to a satisfactory degree.
      D It meets the criterion very well.
      E It meets the criterion exceptionally well, with little to no room for improvement.
    choices: ["A", "B", "C", "D", "E"]
    model_name: gpt-4
    description: "Evaluate the accuracy of the bot's response in relation to the user's input"
    scale_description: "0-4"
    choice_scores:
      A: 0
      B: 1
      C: 2
      D: 3
      E: 4


selection_strategy:
  ahp_selection:
    criteria:
      # - openai_elo_evaluator
      - "openai_prompt_based_evaluator: similarity"
      - "openai_prompt_based_evaluator: accuracy"
      - average_token_usage
      - average_latency
    criteria_maximization:
      # openai_elo_evaluator: true
      "openai_prompt_based_evaluator: similarity": true
      "openai_prompt_based_evaluator: accuracy": true
      average_latency: false
      average_token_usage: false
    criteria_weights:
      # openai_elo_evaluator: 0.4
      "openai_prompt_based_evaluator: similarity": 0.4
      "openai_prompt_based_evaluator: accuracy": 0.4
      average_latency: 0.1
      average_token_usage: 0.1

variations:
  - generator_config:
      diversify: false
      max_tokens: 7000
      number_of_variations: 5
      model_name: gpt-4
      prompt:
        - content: |-

            Your objective is to construct a concise instruction prompt for GPT-4. This prompt will guide GPT-4 in its interactions as a fantasy character for a Weibo auto-reply chatbot tailored to Chinese audiences. 

            Points to emphasize in your instruction:
            - GPT-4 will be role-playing a fantasy character. The specifics of this character – their name, traits, backstory, and nuances – will be provided via placeholders.
            - GPT-4's responses should align with Weibo's conversational style and should be mindful of Chinese cultural and linguistic nuances.
            - At all times, GPT-4 must remain in character, ensuring interactions are genuine, lively, and immersive.
            - The character's personality should be accentuated, including their interests, desires, emotions, and other traits.
            - `{query_context}` acts as GPT-4's reservoir of memory, holding essential background about the character.
            - GPT-4 should not generate or elaborate on the character's story but strictly follow the details provided through placeholders.

            Craft your instruction ensuring GPT-4 understands that it will only provide answers in line with the fantasy character's persona and nothing beyond the scope of the placeholders.
            Craft your instruction ensuring GPT-4 understands that it will only provide answers in line with the Chinese Weibo conversational style, but short
            keep your output crisp: only the prompt, devoid of any extraneous content.

          role: system
        - content: |-

            {CHARACTER_BIO} represent character's shot bio
            {CHARACTER_NAME} represent character name
            {CHARACTER_QUOTES} represent the some of the characer's quotes
            {query_context} represent context about the chracter, given user's reply in the prompt

          role: user
      variables:
        - CHARACTER_BIO
        - CHARACTER_NAME
        - CHARACTER_QUOTES
        - query_context
      output_path: generated_prompt_chinese.pkl
    generator_name: openai_prompt_based_variation_generator

    name:
      chatbot_prompt

      # Variations allow for dynamic content during experiments.
      # They are identified by a globally unique name. For example, in your code,
      # you might reference a variation by its name, like:
      # variation = StringWrapper("hello", 'test_experiment')
      # In this config, you would define the variations associated with that name.
