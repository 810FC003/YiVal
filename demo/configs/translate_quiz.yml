custom_function: demo.translate_quiz.translate_quiz
description: Generated experiment config
dataset:
  data_generators:
    openai_prompt_data_generator:
      chunk_size: 100000
      diversify: true
      # model_name specify the llm model , e.g. a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3
      model_name: gpt-4
      prompt:
          "Please provide a concrete and realistic test case as a dictionary for function invocation using the ** operator.
          Only include parameters, excluding description and name.
          Ensure it's succinct and well-structured.
          **Only provide the dictionary.**"
      input_function:
        description:
          Given an English sentence and corresponding standard Chinese translation, then translate it into Chinese.
        name: translation_english_to_chinese
        parameters:
          teacher_quiz: str
      number_of_examples: 2
      output_path: null
  source_type: machine_generated

variations:
  - name: task
    variations:
      - instantiated_value: Translate this English sentence to Chinese 
        value: Translate this English sentence to Chinese 
        value_type: str
        variation_id: null

# evaluators: # compare teacher's answer(expect result) generate with data generator and student's answer generated with custom func?
#   - evaluator_type: individual
#     metric_calculators:
#       - method: AVERAGE
#     name: rouge_evaluator

# improver: # different from our default imporver, how to connect openai_finetune_utils to the improver?
#   name: 
