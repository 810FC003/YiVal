custom_function: demo.translate_quiz.translate_quiz
description: Generated experiment config
dataset:
  data_generators:
    openai_prompt_data_generator:
      chunk_size: 100000
      diversify: true
      # model_name specify the llm model , e.g. a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3
      model_name: gpt-4
      prompt:
          "Please provide a concrete and realistic test case as a dictionary for function invocation using the ** operator.
          Only include parameters, excluding description and name.
          Ensure it's succinct and well-structured.
          **Only provide the dictionary.**"
      input_function:
        description:
          The current task requires assessing the model's translation ability. Please provide a complex and vivid sentence in English and the corresponding Chinese, which can fully evaluate the translation ability from English to Chinese.
        name: translation_english_to_chinese
        parameters:
          teacher_quiz: str
          teacher_answer: str
      expected_param_name: teacher_answer
      number_of_examples: 5
      output_path: null
  source_type: machine_generated


variations:
  - name : model_name
    variations:
      - instantiated_value: a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3
        value: a16z-infra/llama-2-13b-chat:9dff94b1bed5af738655d4a7cbcdcde2bd503aa85c94334fe1f42af7f3dd5ee3
        value_type: str
        variation_id: null

evaluators:
  - evaluator_type: individual
    name: bertscore_evaluator
    metric_calculators:
      - method: AVERAGE
    display_name: p
    indicator: p
  - evaluator_type: individual
    name: bertscore_evaluator
    metric_calculators:
      - method: AVERAGE
    display_name: r
    indicator: r
  - evaluator_type: individual
    name: bertscore_evaluator
    metric_calculators:
      - method: AVERAGE
    display_name: f
    indicator: f

selection_strategy:
  ahp_selection:
    criteria:
      - "bertscore_evaluator: p"
      - "bertscore_evaluator: r"
      - "bertscore_evaluator: f"
    criteria_maximization:
      "bertscore_evaluator: p": true
      "bertscore_evaluator: r": true
      "bertscore_evaluator: f": true
    criteria_weights:
      "bertscore_evaluator: p": 0.33
      "bertscore_evaluator: r": 0.33
      "bertscore_evaluator: f": 0.33
    normalize_func: null