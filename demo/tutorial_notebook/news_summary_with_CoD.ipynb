{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPr8k012A73n"
      },
      "source": [
        "# make sure your python version >= 3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVP4wjWTA-J2"
      },
      "source": [
        "# Install Yival with git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WEFjY14-MeP",
        "outputId": "fad93a71-00c2-42d4-ffa7-a74683025c34"
      },
      "outputs": [],
      "source": [
        "%pip install yival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M971BMpUBI-e"
      },
      "source": [
        "# Configure your OpenAI API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb2zqa0uBPAW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY']= ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Wq1Oeb-bSY"
      },
      "source": [
        "# Guardrails evaluation yml\n",
        "## Dataset\n",
        "The experiment sources its data from a CSV file which storage 80 leetcode problems (leetcode_problems.csv). The data source type is defined as a dataset.\n",
        "\n",
        "## Custom Function\n",
        "The custom function `demo.guardrails.run_leetcode.run_leetcode` is utilized in this experiment. This function addresses Leetcode problems by either utilizing Guardrails' wrapper or directly employing the ChatCompletion interface. It then returns the corresponding Python code as the result.\n",
        "\n",
        "## Variations\n",
        "In this experiment, we will explore two different variations: **`use_guardrails`** and **`gpt`**. **`use_guardrails`** indicates the use of Guardrails method in the experiment, while **`gpt`** signifies the absence of Guardrails method. These two variations will impact the outcomes and performance of the experiment.\n",
        "\n",
        "## Evaluators\n",
        "The experiment will utilize an evaluator named **`python_validation_evaluator`** to assess the model's performance. This assessment hinges on the successful execution of the generated Python code. It will compute an average score as the evaluation outcome.\n",
        "\n",
        "```\n",
        "custom_function: demo.guardrails.run_leetcode.run_leetcode\n",
        "dataset:\n",
        "  file_path: demo/guardrails/data/leetcode_problems.csv\n",
        "  reader: csv_reader\n",
        "  source_type: dataset\n",
        "description: Generate the expected results for the Leetcode problems.\n",
        "evaluators:\n",
        "  - evaluator_type: individual\n",
        "    matching_technique: includes\n",
        "    metric_calculators:\n",
        "      - method: AVERAGE\n",
        "    name: python_validation_evaluator\n",
        "\n",
        "variations:\n",
        "  - name: use_guardrails\n",
        "    variations:\n",
        "      - instantiated_value: \"use_guardrails\"\n",
        "        value: \"use_guardrails\"\n",
        "        value_type: str\n",
        "        variation_id: null\n",
        "      - instantiated_value: \"gpt\"\n",
        "        value: \"gpt\"\n",
        "        value_type: str\n",
        "        variation_id: null\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96t7abbdSqT_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('../..')\n",
        "\n",
        "!yival run demo/configs/guardrails_leetcode.yml --async_eval=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCso4fmyA2h_"
      },
      "source": [
        "# Summary Result\n",
        "\n",
        "## Detailed Test Result\n",
        "Here, we utilize 2 news article and to obtain and evaluate responses.\n",
        "\n",
        "![image](https://github.com/uni-zhuan/uni_CDN/blob/master/picture/Yival/iShot_2023-10-08_14.31.36.png?raw=true)\n",
        "\n",
        "\n",
        "## Human Rating\n",
        "Using Yival's human rating method, you can manually evaluate the results of CoD.\n",
        "\n",
        "![image](https://github.com/uni-zhuan/uni_CDN/blob/master/picture/Yival/iShot_2023-10-08_14.40.05.png?raw=true)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
