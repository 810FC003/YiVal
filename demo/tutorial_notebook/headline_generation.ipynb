{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make sure your python version >= 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Yival with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']= 'XXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Generation demo\n",
    "\n",
    "This configuration file outlines the setup for an experiment involving the generation of landing page headlines for tech startups. The experiment uses GPT-4, an OpenAI model, and is structured as follows:\n",
    "\n",
    "## Dataset\n",
    "The data for the experiment is machine-generated. The prompt  inside instructs GPT-4 to generate a landing page headline for a given tech startup. The data generator will create two examples for each startup.\n",
    "\n",
    "## Variations\n",
    "The experiment includes variations of the task, which are also generated by GPT-4. The task is to generate a concise instruction prompt for a given tech startup.\n",
    "\n",
    "## Evaluators\n",
    "The experiment uses an individual evaluator type, which assesses the clarity of the generated headline. The evaluator uses a scale from \"A\" (fails to meet the criterion) to \"E\" (meets the criterion exceptionally well).\n",
    "\n",
    "## Selection Strategy\n",
    "The experiment uses the Analytic Hierarchy Process (AHP) for selection, which considers multiple criteria such as clarity of the headline, average token usage, and average latency. The weights assigned to these criteria are 0.6, 0.2, and 0.2 respectively. The results are normalized using the z-score method.\n",
    "```\n",
    "custom_function: demo.headline_generation.headline_generation\n",
    "description: Generated experiment config\n",
    "dataset:\n",
    "  data_generators:\n",
    "    openai_prompt_data_generator:\n",
    "      chunk_size: 100000\n",
    "      diversify: true\n",
    "      prompt:\n",
    "          \"Please provide a concrete and realistic test case as a dictionary for function invocation using the ** operator.\n",
    "          Only include parameters, excluding description and name.\n",
    "          Ensure it's succinct and well-structured.\n",
    "          **Only provide the dictionary.**\"\n",
    "      input_function:\n",
    "        description:\n",
    "          Given an tech startup business, generate a corresponding landing\n",
    "          page headline\n",
    "        name: headline_generation_for_business\n",
    "        parameters:\n",
    "          tech_startup_business: str\n",
    "      number_of_examples: 2\n",
    "      openai_model_name: gpt-4\n",
    "      output_path: null\n",
    "  source_type: machine_generated\n",
    "\n",
    "variations:\n",
    "  - name: task\n",
    "    variations:\n",
    "      - instantiated_value: Generate landing page headline for\n",
    "        value: Generate landing page headline for\n",
    "        value_type: str\n",
    "        variation_id: null\n",
    "    generator_name: openai_prompt_based_variation_generator\n",
    "    generator_config:\n",
    "      openai_model_name: gpt-4\n",
    "      number_of_variations: 2\n",
    "      diversify: true\n",
    "      variables: null\n",
    "      prompt:\n",
    "        - content: |-\n",
    "            Your objective is to construct a concise instruction prompt for GPT-4.\n",
    "            Task: Given an tech startup business, generate one corresponding landing page headline which is really attracting to all people.\n",
    "            The name of the company will be add to the end of the prompt.\n",
    "            keep your output crisp: only the prompt, devoid of any extraneous content.\n",
    "\n",
    "          role: system\n",
    "\n",
    "evaluators:\n",
    "  - evaluator_type: individual\n",
    "    metric_calculators:\n",
    "      - method: AVERAGE\n",
    "    name: openai_prompt_based_evaluator\n",
    "    display_name: clear\n",
    "    prompt: |-\n",
    "      You are assessing a submitted answer on a given task based on a criterion. Here is the data:\n",
    "      - Task: Given an tech startup business, generate one corresponding landing page headline\n",
    "      - Does the headline clearly communicate what the startup does or what problem it solves?\n",
    "        It should be immediately clear to anyone who reads the headline what the startup's purpose is.\n",
    "        A lack of clarity can lead to confusion and may discourage potential users or investors.\n",
    "      [Input]: {tech_startup_business}\n",
    "      [Result]: {raw_output}\n",
    "      Answer the question by selecting one of the following options:\n",
    "      A It fails to meet the criterion at all.\n",
    "      B It somewhat meets the criterion, but there is significant room for improvement.\n",
    "      C It meets the criterion to a satisfactory degree.\n",
    "      D It meets the criterion very well.\n",
    "      E It meets the criterion exceptionally well, with little to no room for improvement.\n",
    "    choices: [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "    openai_model_name: gpt-4\n",
    "    description: \"evaluate the quality of the landing page headline\"\n",
    "    scale_description: \"0-4\"\n",
    "    choice_scores:\n",
    "      A: 0\n",
    "      B: 1\n",
    "      C: 2\n",
    "      D: 3\n",
    "      E: 4\n",
    "\n",
    "\n",
    "selection_strategy:\n",
    "  ahp_selection:\n",
    "    criteria:\n",
    "      - \"openai_prompt_based_evaluator: clear\"\n",
    "      - average_token_usage\n",
    "      - average_latency\n",
    "    criteria_maximization:\n",
    "      \"openai_prompt_based_evaluator: clear\": true\n",
    "      average_latency: false\n",
    "      average_token_usage: false\n",
    "    criteria_weights:\n",
    "      \"openai_prompt_based_evaluator: clear\": 0.6\n",
    "      average_latency: 0.2\n",
    "      average_token_usage: 0.2\n",
    "    normalize_func: \"z-score\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "!yival run demo/configs/headline_generation.yml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
