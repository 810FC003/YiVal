{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto reply use case\n",
    "## In this use case, our demo will provide a response based on the conversation prompted. We also use Langchain for information retrieval, allowing the model to access more information for a more informed reply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make sure your python version >= 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Yival with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install yival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']= 'XXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Reply Demo\n",
    "\n",
    "This configuration file outlines the setup for an experiment involving the generation of auto replies for a Weibo chatbot role-playing as a predefined fantasy character. The experiment uses GPT-4, an OpenAI model, and is structured as follows:\n",
    "\n",
    "## Dataset\n",
    "The data for the experiment is sourced from a CSV file (`test_data.csv`). The source type is defined as a dataset.\n",
    "\n",
    "## Custom Function\n",
    "The custom function `demo/auto_reply.reply.reply` is used in this experiment.\n",
    "\n",
    "## Variations\n",
    "The experiment includes variations of the task, which are also generated by GPT-4. The task is to construct a concise instruction prompt for GPT-4, guiding it to interact as a fantasy character for a Weibo auto-reply chatbot tailored to Chinese audiences.\n",
    "\n",
    "## Evaluators\n",
    "The experiment uses an \"all\" evaluator type, which assesses whether GPT-4's responses align with character traits, backstory, and Chinese cultural context. There are no specific metric calculators defined for this evaluator.\n",
    "\n",
    "## Selection Strategy\n",
    "The experiment uses the Analytic Hierarchy Process (AHP) for selection, which considers multiple criteria such as evaluator score, average token usage, and average latency. The weights assigned to these criteria are 0.6, 0.2, and 0.2 respectively. The results are normalized using the z-score method.\n",
    "\n",
    "```\n",
    "custom_function: demo/auto_reply.reply.reply\n",
    "dataset:\n",
    "  file_path: demo/data/test_data.csv\n",
    "  reader: csv_reader\n",
    "  source_type: dataset\n",
    "description: Generated experiment config\n",
    "\n",
    "evaluators:\n",
    "  - evaluator_type: all\n",
    "    input_description: Guide GPT-4 to role-play as a predefined fantasy character for a Weibo auto-reply chatbot, ensuring responses align with character traits, backstory, and Chinese cultural context.\n",
    "    metric_calculators: []\n",
    "    name: openai_elo_evaluator\n",
    "    openai_model_name: gpt-4\n",
    "\n",
    "selection_strategy:\n",
    "  ahp_selection:\n",
    "    criteria:\n",
    "      - openai_elo_evaluator\n",
    "      - average_token_usage\n",
    "      - average_latency\n",
    "    criteria_maximization:\n",
    "      openai_elo_evaluator: true\n",
    "      average_latency: false\n",
    "      average_token_usage: false\n",
    "    criteria_weights:\n",
    "      openai_elo_evaluator: 0.6\n",
    "      average_latency: 0.2\n",
    "      average_token_usage: 0.2\n",
    "\n",
    "variations:\n",
    "  - generator_config:\n",
    "      diversify: false\n",
    "      max_tokens: 7000\n",
    "      number_of_variations: 5\n",
    "      model_name: gpt-4\n",
    "      prompt:\n",
    "        - content: |-\n",
    "\n",
    "            Your objective is to construct a concise instruction prompt for GPT-4. This prompt will guide GPT-4 in its interactions as a fantasy character for a Weibo auto-reply chatbot tailored to Chinese audiences. \n",
    "\n",
    "            Points to emphasize in your instruction:\n",
    "            - GPT-4 will be role-playing a fantasy character. The specifics of this character – their name, traits, backstory, and nuances – will be provided via placeholders.\n",
    "            - GPT-4's responses should align with Weibo's conversational style and should be mindful of Chinese cultural and linguistic nuances.\n",
    "            - At all times, GPT-4 must remain in character, ensuring interactions are genuine, lively, and immersive.\n",
    "            - The character's personality should be accentuated, including their interests, desires, emotions, and other traits.\n",
    "            - `{query_context}` acts as GPT-4's reservoir of memory, holding essential background about the character.\n",
    "            - GPT-4 should not generate or elaborate on the character's story but strictly follow the details provided through placeholders.\n",
    "\n",
    "            Craft your instruction ensuring GPT-4 understands that it will only provide answers in line with the fantasy character's persona and nothing beyond the scope of the placeholders.\n",
    "            Craft your instruction ensuring GPT-4 understands that it will only provide answers in line with the Chinese Weibo conversational style, but short\n",
    "            keep your output crisp: only the prompt, devoid of any extraneous content.\n",
    "\n",
    "          role: system\n",
    "        - content: |-\n",
    "\n",
    "            {CHARACTER_BIO} represent character's shot bio\n",
    "            {CHARACTER_NAME} represent character name\n",
    "            {CHARACTER_QUOTES} represent the some of the characer's quotes\n",
    "            {query_context} represent context about the chracter, given user's reply in the prompt\n",
    "\n",
    "          role: user\n",
    "      variables:\n",
    "        - CHARACTER_BIO\n",
    "        - CHARACTER_NAME\n",
    "        - CHARACTER_QUOTES\n",
    "        - query_context\n",
    "      output_path: generated_prompt_chinese.pkl\n",
    "    generator_name: openai_prompt_based_variation_generator\n",
    "\n",
    "    name:\n",
    "      chatbot_prompt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "!yival run demo/configs/auto_prompts_config.yml"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
